{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLA and neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural network is a composition of a set of functions\n",
    "\n",
    "- These functions are typically linear and non-linear\n",
    "- You can represent this network as a computational graph\n",
    "- This representation helps a lot in [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a gradient and how we can compute it automatically?\n",
    "\n",
    "- If $f$ depends on vector or matrix and returns a scalar, then its gradient is a vector or matrix, whose elements are derivatives of output w.r.t. corresponding element of the input\n",
    "- Example: $f(x) = x^{\\top}Ax + b^{\\top}x$. What is a gradient of this simple quadratic function?\n",
    "- Elementwise example: $f(x) = x^2$. What is a Jacobi matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-7.4333, grad_fn=<SubBackward0>)\n",
      "-7.433262825012207\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "n = 5\n",
    "A = torch.randn((n, n), requires_grad=True)\n",
    "b = torch.randn((n,))\n",
    "x = torch.randn((n,), requires_grad=True)\n",
    "\n",
    "f = 0.5 * x @ A @ x - b @ x\n",
    "f.backward()\n",
    "print(f)\n",
    "print(f.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0997, -3.0563,  5.3020,  1.2093, -4.0723])\n",
      "tensor([-0.0997, -3.0563,  5.3020,  1.2093, -4.0723])\n",
      "tensor([-0.0997, -3.0563,  5.3020,  1.2093, -4.0723])\n"
     ]
    }
   ],
   "source": [
    "manual_grad_x = 0.5 * (A + A.t()) @ x - b\n",
    "\n",
    "print(manual_grad_x.data)\n",
    "print(x.grad.data)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5078,  0.7830, -1.0070, -0.1017,  0.4298],\n",
      "        [ 0.7830,  1.2074, -1.5528, -0.1568,  0.6628],\n",
      "        [-1.0070, -1.5528,  1.9971,  0.2017, -0.8524],\n",
      "        [-0.1017, -0.1568,  0.2017,  0.0204, -0.0861],\n",
      "        [ 0.4298,  0.6628, -0.8524, -0.0861,  0.3639]])\n",
      "tensor([[ 0.5078,  0.7830, -1.0070, -0.1017,  0.4298],\n",
      "        [ 0.7830,  1.2074, -1.5528, -0.1568,  0.6628],\n",
      "        [-1.0070, -1.5528,  1.9971,  0.2017, -0.8524],\n",
      "        [-0.1017, -0.1568,  0.2017,  0.0204, -0.0861],\n",
      "        [ 0.4298,  0.6628, -0.8524, -0.0861,  0.3639]])\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "manual_grad_A = 0.5 * torch.ger(x, x)\n",
    "\n",
    "print(manual_grad_A.data)\n",
    "print(A.grad.data)\n",
    "print(torch.norm(manual_grad_A.data - A.grad.data).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How these operations relate to the neural networks?\n",
    "\n",
    "- Supervised learning and other problem statements \n",
    "- We have data, we have labels\n",
    "- Neural network $\\approx$ **complex** composition of **simple** functions, which recovers label based on feeded data\n",
    "- To train target function a.k.a. loss function, one of the stochatic first-order method is used\n",
    "- Therefore, we need gradients!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./pytorch_logo.png\">\n",
    "\n",
    "- It is a convenient framework for constructing and training neural networks\n",
    "- It implements dynamic computational graph\n",
    "- You just implement function that computes required quantities, call it with some arguments and you can get the gradient of this function for this arguments for free!\n",
    "- Sparse matrices support is still under development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What operations are typically used as an ingredients of NN?\n",
    "\n",
    "- Matrix by vector (or matrix) multiplication and additions - fully-connected layers\n",
    "- Elementwise operations are non-linearities\n",
    "- Local operations like [MaxPooling](https://deepai.org/machine-learning-glossary-and-terms/max-pooling)\n",
    "- Different reduction operations to make a scalar from vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- NLA is a basis of neural networks as well as of other computational techniques\n",
    "- Moderm frameworks (PyTorch, JAX, TF, etc) computes gradients automatically\n",
    "- [Colab](https://colab.research.google.com/) allows you to perform simple tests and improve your understanding the topic"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
